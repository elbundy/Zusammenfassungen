\documentclass{scrartcl}

\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{gensymb}

\usepackage[a4paper, left=1cm, right=1cm, top=2cm, bottom=3cm]{geometry}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%
\DeclarePairedDelimiter\inner{\langle}{\rangle}%
\newcommand{\ffrac}[2]{\ensuremath{\frac{\displaystyle #1}{\displaystyle #2}}}

\begin{document}
\section{Fourier Transform in a Nutshell}
Key Idea: Compare signal with sinusoids of various frequencies and phases. 

\subsection{Continuous Fourier Transform}
\subsubsection*{Analog Signals}
\begin{itemize}
    \item
        Time as well as amplitude are continuous, real-valued parameters
    \item
        Can be modeled as a function $f: \mathbb{R} \rightarrow \mathbb{R}$, which assigns to each time point $t \in \mathbb{R}$ an amplitude value $f(t) \in \mathbb{R}$
\end{itemize}
\subsubsection*{Sinussoidal signal}
\begin{itemize}
    \item
        Function $g: \mathbb{R} \rightarrow \mathbb{R}$ defined by $g(t) = A sin(2\pi(\omega t - \varphi))$
    \item
        $A$ corresponds to the \textbf{amplitude}, $\omega$ to the \textbf{frequency} and $\varphi$ to the \textbf{phase}
    %\item
        %frequency is measured in Hz, phase in normalized radians (1 corresonds to an angle of 360\degree)
    \item
        In Fourier analysis: prototype oscillations that are normalized with regard to their power by setting $A = \sqrt2$
    \item
        Thus for each frequency parameter $\omega$ and phase parameter $\varphi$ we obtain a sinusoid 
        $$cos_{\omega, \varphi}(t) = \sqrt2 cos(2\pi(\omega t - \varphi))$$
    \item
        Phase parameter only has to be considered for $\varphi \in [0, 1)$
\end{itemize}
\subsubsection*{Computing Similarity with Integrals}
\begin{itemize}
    \item
        $\int_{t \in \mathbb{R}} f(t)g(t) dt$
\end{itemize}
\subsubsection*{First Definition of the Fourier Transform}
\begin{itemize}
    \item
        For a fixed frequency $\omega \in \mathbb{R}$, we define
        $$d_\omega = max_{\varphi \in [0,1)} (\int_{t \in \mathbb{R}} f(t) cos_{\omega, \varphi}(t) dt)$$
        $$\varphi_\omega = argmax_{\varphi \in [0,1)} (\int_{t \in \mathbb{R}} f(t) cos_{\omega, \varphi}(t) dt)$$
    \item
        \textbf{Fourier Transform:} "collection" of all coefficients $d_\omega$ and $\varphi_\omega$ for $\omega \in \mathbb{R}$
\end{itemize}
\subsubsection*{Complex Numbers}
\begin{itemize}
    \item
        Extends real numbers by introducing the imaginary number $i = \sqrt{-1}$ with $i^2 = -1$
    \item
        Gets written as $c = a + ib$ 
    \item
        Set of complex numbers $\mathbb{C}$ can be thought of as a two-dimensinal plane, where horizontal dimension corresponds to real part and vertical dimension to imaginary part (so $c = a+ib$ has Cartesian coordinates $(a,b)$)
    \item
        \textbf{Polar coordinates:} Complex number c is described by absolute value $\abs{c}$ (distance from origin) and angle $\gamma$ between the positive horizontal axis and the line from the origin to $c$
    \item
        Deriving polar coordinates:
        $$ \abs{x} = \sqrt{a^2 + b^2} $$
        $$ \gamma = atan2(b, a) $$
    \item
        Regaining complex number:
        $$ exp(i\gamma) = cos(\gamma) + isin(\gamma) $$
        $$ c = \abs{c} exp(i\gamma)$$
\end{itemize}
\subsubsection*{Complex Definition of the Fourier Transform}
\begin{itemize}
    \item
        Idea: encode $d_\omega$ and $\varphi_\omega$ by a single complex number
    \item
        $ c_\omega = \ffrac{d_\omega}{\sqrt{2}} exp(2\pi i(-\varphi_\omega))$
    \item
        $\hat{f}(\omega) = c_\omega$
    \item     
        $\hat{f}$ is referred to as the Fourier transform of f, $\hat{f}(\omega) = c_\omega$ are called the Fourier coefficients
    \item
        Fourier transform can be computed by

        \begin{align*}
            \hat{f}(\omega) = & \int_{t \in \mathbb{R}} f(t) exp(-2\pi i \omega t) dt\\
            & \int_{t \in \mathbb{R}} f(t) cos(-2\pi \omega t) dt + i \int_{t \in \mathbb{R}} f(t) sin(-2 \pi \omega t) dt
        \end{align*}
    \item
        Real part is obtained by comparing $f$ with a cosine function, the imaginary part by comparing $f$ with a sine function
    \item
        $\abs{\hat{f}(\omega)}$ is called the \textbf{magnitude Fourier transform}
    \item
        With $\abs{\hat{f}(\omega)}$ and $\gamma_\omega$ being the polar coordinates of $\hat{f}(\omega)$:
        $$d_\omega = \sqrt{2} \abs{\hat{f}(\omega)}$$
        $$\varphi_\omega = -\ffrac{\gamma_\omega}{2\pi}$$
\end{itemize}
\subsubsection*{Fourier Representation}
\begin{itemize}
    \item
        Signal reconstruction:
        \begin{align*}
            f(t) & = \int_{\omega \in \mathbb{R}_{\geq 0}} d_\omega \sqrt{2} cos(2 \pi (\omega t - \varphi_\omega))d\omega\\
            & = \int_{\omega \in \mathbb{R}} c_\omega exp(2\pi i \omega t)d \omega
        \end{align*}
        
\end{itemize}
\subsection{Discrete Fourier Transform}
\subsubsection*{Equidistant sampling}
\begin{itemize}
    \item
        Digitization: Converting analog signals into finite representations (analog-to-digital conversion) 
    \item
        Given an analog signal $f: \mathbb{R} \rightarrow \mathbb{R}$ and a positive real number $T > 0$, one defines a function $x: \mathbb{Z} \rightarrow \mathbb{Z}$ by setting $x(n) = f(nT)$
    \item
        Since $x$ is only defined on a discrete set of time points, it is also refered to as a \textbf{discrete-time} (DT) signal
    \item
        $x(n)$ is called a sample, taken at time $t = nT$
    \item
        $F_s = 1/T$ is called sampling rate
    \item
        \textbf{Sampling theorem:} Original signal $f$ can be reconstructed perfectly from its sampled version $x$, if $f$ does not contain any frequencies higher than
        $$ \Omega = F_s/2 = 1/(2T) Hz $$
    \item
        We say $f$ is an \textbf{$\Omega$-bandlimited} signal, $\Omega$ is known as the \textbf{Nyquist frequenct}
    \item
        In the case $f$ contains higher frequencies, sampling may cause artifacts referred to as \textbf{aliasing}
\end{itemize}
\subsubsection*{Discrete Fourier Transform}
\begin{itemize}
    \item
        Sample the sinusoudal prototype oscillation in the same fashion as the signal and multiply the two sampled functions in a pointwise fashion (sampled product)
    \item
        Integration in the continuos case becomes summation in the discrete case, where the summands need to be weighted by the sampling period $T$
        $$\sum_{n \in \mathbb{Z}} T f(nT) exp(-2\pi i \omega n T) \approx \hat{f}(\omega)$$
    \item
        "Approximation of integral area via sum of rectangular shapes" (\textbf{Riemann sum})
    \item
        Substituting $T = 1$ and $f(nT) = x(n)$ yields
        $$ \hat{x}(\omega) = \sum_{n \in \mathbb{Z}} x(n) exp(-2\pi i \omega n) $$
    \item
        To recover relation to Fourier transform $\hat{f}$, one needs to know the sampling period $T$
        $$ \hat{x}(\omega) \approx \ffrac{1}{T} \hat{f}(\ffrac{\omega}{T})$$
    \item
        $\omega = 1/2$ for $\hat{x}$ corresponds to the Nyquist frequency $\Omega = 1/(2T)$ of the sampling process
    \item
        Two \textbf{problems} for computation: 
        \begin{enumerate}
            \item
                Infinite number of summands ($\sum_{n \in \mathbb{Z}}$)
            \item
                $\omega$ is a continuos parameter
        \end{enumerate}
    \item
        \textbf{Solutions:}
        \begin{enumerate}
            \item
                Assumption: most relevant information of $f$ is limited to a certain duration in time (e.g. song only lasts a few minutes). This means that we only need to consider a finite amount of samples $x(0),\dots,x(N-1)$ and the sum becomes finite.
            \item
                One computes the Fourier transform only for a finite number of frequencies ("sampling frequency axis" by considering frequencies $\omega = k/M$ for some suitable $M \in \mathbb{N}$ and $k \in [0: M-1]$). Often $N=M$ gets choosen.
        \end{enumerate}
    \item
        \textbf{Discrete Fourier Transform:}
        $$ X(k) = \hat{x}(k/N) = \sum_{n=0}^{N-1} x(n) exp(-2\pi i k n/N)$$
        defined for integers $k \in [0: M-1] = [0 : N-1]$
    \item
        The frequency $\omega$ of $\hat{x}$ corresponds to $\omega/T$ of $\hat{f}$, so $F_{coef}(k) = \ffrac{k}{N \cdot T} = \ffrac{k \cdot F_s}{N}$
    \item
        Upper half of coefficients are redundant, one only needs to consider the coefficients $X(k)$ for $k \in [0: \lfloor N/2 \rfloor]$
    \item
        To compute all coefficients, one needs $N^2$ operations
    \item
        \textbf{Fast Fourier Transform}: Recursive computation, works particularly well in the case of N is a power of two, $Nlog_2N$ operations
\end{itemize}

\subsection{Short-Time Fourier Transform}
\begin{itemize}
    \item
        Instead of considering the entire signal, the main idea of the STFT is to consider only a small section of the signal 
    \item
        One fixes a so-called \textbf{window function}, which is a function that is nonzero for only a short period of time
    \item
        The original signal is multiplied with the window funtion to yield a \textbf{windowed signal}
    \item
        To obtain frequency information at different time instances, one shifts the window function across time and computes a FT for each of the resulting windows
    \item
        STFT reflects not only the properties of original signal but also those of the window function (e.g. rectangular windows typically introduce "ripple" artifacts)
    \item
        Parameters:
        \begin{itemize}
            \item
                Real-valued DT signal $x: \mathbb{Z} \rightarrow \mathbb{R}$ obtained with sampling rate $F_s$
            \item
                Sampled window function $w: [0:N-1] \rightarrow \mathbb{R}$ of length $N \in \mathbb{N}$
            \item
                Hop size $H \in \mathbb{N}$ specified in samples and determining the step size in which the window is to be shifted across the signal (most of the time $H = N/2$)
        \end{itemize}
    \item
        \textbf{Discrete STFT $X$:}
        $$X(m,k) = \sum_{n=0}^{N-1} x(n+mH)w(n)exp(-2\pi i k n/N)$$
        with $m \in \mathbb{Z}$ and $k \in [0:K]$
    \item
        The number $K = N/2$ is the frequency index corresponding to the Nyquist frequency
    \item
        $X(m,k)$ denotes the $k^{th}$ Fourier coefficient for the $m^{th}$ time frame
    \item
        For each fixed time frame $m$, one obtains a spectral vector of size $K+1$. The computation of each such spectral vector amounts to a DFT of size $N$, which can be done efficiently using the FFT
    \item
        $T_{coef}(m) = \ffrac{m \cdot H}{F_s}$ (in seconds) and $F_{coef}(k) = \ffrac{k \cdot F_s}{N}$ (in Hertz)
    \item
        \textbf{Spectrogram}: two-dimensional representation of the squared magnitude of the STFT
        $$Y(m,k) = \abs{X(m,k)}^2$$
\end{itemize}

\newpage

\section{Music Synchronization}
\begin{itemize}
    \item
        Task: For a given position in one representation of a piece of music, determine the corresponding position within another representation (audio, midi or sheet music in general)
    \item
        Two main steps:
        \begin{enumerate}
            \item
                Transform the two representations into sequence of suitable feature vectors\\
                (large degree of robustness to variations that are to be left unconsidered for the task at hand vs. capture enough characteristic information to accomplish the given task) $\rightarrow$ chroma-based features
            \item
                Bring derived feature sequences into temporal correspondence
        \end{enumerate}
\end{itemize}
\subsection*{Dynamic Time Warping}
\begin{itemize}
    \item
        Two chroma vector sequences $X = (x_1, x_2, \dots, x_N)$ and $Y = (y_1, y_2, \dots, y_M)$
    \item
        Goal: find non-linear alignment between the elements of the two sequences in order to compensate differences in tempo
    \item
        Can be achieved by skipping certain elements of a sequence or by using ceratin elements more than once
\end{itemize}
\subsubsection*{Basic Approach}
\begin{itemize}
    \item
        local cost measure or distance measure $c:\mathcal{F} \times \mathcal{F} \rightarrow \mathbb{R}$     
        \begin{itemize}
            \item
                Euclidian distance: $\norm{x - y}$
            \item
                Cosine distance: $1 - \ffrac{\inner{x, y}}{\norm{x} \cdot \norm{y}}$\\
                Inner product corresponds to cosine of angle between vectors $x$ and $y$\\
                Comparison only considers the energy distributions across the twelve chroma bands and disregards actual local energy\\
                Can be computed efficiently using a simple matrix multiplication
        \end{itemize}
    \item
        Evaluating the local cost measure for each par of elements in $X$ and $Y$, one obtains a cost matrix $C \in \mathbb{R}^{N \times M}$ defined by
        $C(n,m) = c(x_n, y_m)$
    \item
        Goal: find 'valley' of low cost within the cost matrix $C$
\end{itemize}
\subsubsection*{Warping Path}
\begin{itemize}
    \item
        $(N,M)$-warping path of length $L$ is a sequence $P=(p_1, ..., p_L)$ with $p_l = (n_l, m_l)$ satisfying the following three conditions:
        \begin{enumerate}
            \item
                Boundary condition: $p_1 = (1,1)$ and $p_L = (N,M)$
            \item
                Monotonicity condition: $n_1 \leq n_2 \leq \dots \leq n_L$ and $m_1 \leq m_2 \leq \dots \leq m_L$
            \item
                Step size condition: $p_{l+1} - p_l \in \{(1,0), (0,1), (1,1)\}$
        \end{enumerate}
        (3. implies 2.)
    \item
        Warping path assigns element $x_{n_l}$ to element $y_{n_l}$
\end{itemize}
\subsection*{Optimal Warping Path and DTW Distance}
\begin{itemize}
    \item 
        total cost $c_P(X,Y)$ of a warping path $P$ between two sequences $X$ and $Y$ with respects to the local cost measure $c$ is defined as
        $$c_P(X,Y) = \sum_{l=1}^{L}c(x_{n_l}, y_{m_l}) = \sum_{l=1}^{L} C(n_l, m_l)$$
    \item
        DTW distance is defined as the total cost of an optimal $(N,M)$-warping path $P^*$:
        $$DTW(X,Y) = c_{P^*}(X,Y) = min\{c_P(X,Y)|\text{P is an (N,M)-warping path}\}$$
    \item
        there may be multiple optimal warping paths, e.g. when cost matrix is zero everywhere
\end{itemize}

\subsection*{Dynamic Programming Algorithm}
\begin{itemize}
    \item
        Naive approach: compute all possible warping paths and then take minimal cost one
    \item
        Problem: Number of different $(N,M)$-warping paths is exponential in $N$ and $M$
    \item
        Better: Dynamic Programming with $O(NM)$ complexity
    \item
        Basic idea is to break down problem into simpler subproblems and then to combine the solutions
    \item
        Here this means to find optimal warping paths for truncated subsequences of $X$ and $Y$ \\
    \item
        $X(1:n) = (x_1, \dots, x_n)$, $Y(1:m) = (y_1, \dots, y_m)$ and $D(n,m) = DTW(X(1:n), Y(1:m))$
    \item
        Matrix $D$ is called accumulated cost matrix
    \item
        Recursive computation:
        $$D(n,1) = \sum_{k=1}^n C(k,1) \text{for } n \in [1:N]$$
        $$D(1,m) = \sum_{k=1}^m C(1,k) \text{for } n \in [1:N]$$
        $$D(n,m) = C(n,m) + min \{D(n-1, m-1), D(n-1,m), D(n, m-1)\}$$
        for $n\in[2:N]$ and $m\in[2:M]$\\
        Proof: See book
    \item
        Computing $D$ has complexity $O(NM)$
    \item
        Finding optimal warping path: Start with $q_1=(N,M)$ and continue following until $q_L = (1,1)$ reached:
        \begin{itemize}
            \item
                $q_{l+1} = (1, m-1) \text{if } n=1$
            \item
                $q_{l+1} = (n-1, 1) \text{if } n=1$
            \item
                $q_{l+1} = argmin\{D(n-1,m-1), D(n-1, m), D(n,m-1)\}$
        \end{itemize}
    \item
        Problem: still infeasible for large $N$ and $M$
    \item
        Better: Global constraints (Limit region where optimal warping path can be), Multiscale approach
\end{itemize}

\newpage

\section{Music Structure Analysis}
\begin{itemize}
    \item
        Musical parts of popular music: intro, chorus, verse, ...
    \item
        Goal: divide a given music representation into temporal segments that correspond to musical parts and to group these segments into musically meaningful categories (find and understand the relationship between the segments)
    \item
        Musical structure is related to:
        \begin{itemize}
            \item
                Repetition, e.g. recurring patterns of rythmic, harmonic or melodic nature
            \item
                Homogenity, e.g. consistent timber, presence of specific instrument or usage of certain harmonies
            \item
                Novelty (Neuheit): e.g. segment boundaries go along with sudden changes in musical properties such as tempo, dynamics or the musical key
        \end{itemize}
        $\rightarrow$ \textbf{repetition-based}, \textbf{novelty-based} and \textbf{homogenity-based} methods
    \item
        First step: transform the given music recording into a suitable feature representation (trade-off between robustness and expressiveness)
    \item
        Feature types:
        \begin{itemize}
            \item
                chroma features: harmonic and melodic properties
            \item
                mel-frequency cepstral coefficients (MFCCs): instrumentation and timber
            \item
                beat-synchronous features, tempogram: beat and tempo (and rythm)
        \end{itemize}
\end{itemize}
\subsection*{Self-Similarity Matrices}
\begin{itemize}
    \item
        Idea: Convert musical recording into sequence of feature vectors and compare each element with all other elements of the sequence
    \item
        Result: Self-Similarity Matrix (SSM)
    \item
        One crucial property of SSMs is that repetitions typically yield path-like structures, whereas homogeneous regions yield block-like structures\\
    \item
        Similarity measure $s:\mathcal{F} \times \mathcal{F} \rightarrow \mathbb{R}$
        \begin{itemize}
            \item
                simple measure is absolute value of inner product: $s(x,y) = |\inner{x,y}|$
            \item
                if feature vectors are normalized with respects to the Euclidian norm, $s(x,y) \in [0,1]$
        \end{itemize}
    \item
        N-square \textbf{self-similarity matrix} $S\in \mathbb{R}^{N\times N}$ defined by $S(n,m) = s(x_n, x_m)$
    \item
        Diagonal with high values
    \item
        Segment is defined as $\alpha = [s:t]$ with $|\alpha| = t-s+1$ being the length of the segment
    \item
        \textbf{Path} over $\alpha$ of length L is a sequence $P=((n_1, m_1), \dots, (n_L, m_L))$ with
        \begin{itemize}
            \item
                $m_1 = s, m_L=t$ (boundary condition)
            \item
                $(n_{l+1}, m_{l+1}) - (n_l, m_l) \in \Sigma$, $\Sigma = \{(2,1), (1,2), (1,1)\}$
        \end{itemize}
    \item
        For a path one can associate two segments defined by the projections\\
        $\pi_1(P) = [n_1 : n_L]$ and $\pi_2(P) = [m_1 : m_L]$\\
        where the boundary condition enforces $\pi_2(P)=\alpha$. $\pi_1(P)$ is refered to as the \textbf{induced segment}
    \item
        The score $\sigma(P)$ is defined as $\sigma(P) = \sum_{l=1}^{L} S(n_l, m_l)$ (yields measure of similarity between $\alpha$ and the induced segment)
    \item
        \textbf{Block} over $\alpha$ is a subset $B = \alpha' \times \alpha \subseteq [1:N] \times [1:N]$ for some segment $\alpha' = [s', t']$ (N is length of feature sequence)
    \item
        Similarily we define two projections $\pi_1(B) = \alpha'$ and $\pi_2(B) = \alpha$ , where $\alpha'$ is the induced segment
    \item
        Based on paths and blocks we can now consider differnt kinds of similarity relations between segments
    \item
        Segment $\alpha_1$ is \textbf{path-similar} to a segment $\alpha_2$, if there is a path $P$ of high score with $\pi_1(P) = \alpha_1$ and $\pi_2(P) = \alpha_2$
    \item
        Segement $\alpha_1$ is \textbf{block-similar} to $\alpha_2$, if there is a block B of high score with $\pi_1(B) = \alpha_1$ and $\pi_2(B) = \alpha_2$
    \item
        If similarity measure is symmetric, then SSM S and as well as similarity relations are symmetric
    \item
        Transitivity of similarity relation: $\alpha_1$ similar to $\alpha_2$ and $\alpha_2$ similar to $\alpha_4$ $\Rightarrow$ $\alpha_1$ similar to $\alpha_3$ (at least to a certain degree)
    \item
        Consequence: Block-similarity between $\alpha$ and $\alpha'$, and $\alpha$ and $\alpha'$ $\Rightarrow$ Block-similarity between $\alpha$ and $\alpha$\\
    \item
        Algorithmic pipeline:
        \begin{enumerate}
            \item
                The musical signal is transformed into a suitable feature sequence
            \item
                A self-similarity matrix is computed from the feature sequence based on a similarity measure
            \item
                Blocks and paths of high overall score are derived from the SSM. Each block or path defines a pair of similar segments
            \item
                Entire groups of mutually similar segments are formed from the pairwise relations by applying a clustering step
        \end{enumerate}
    \item
        Leaves a lot of freedom and needs to be adjusted to account for particular properties of the underlying type of music and the requirements of the intended application
    \item
        Major challenges arise from the fact that musical parts are rarely repeated in precisely the same way (differ in dynamics, orchestration, articulation, tempo, harmony, melody or any combination of these) $\rightarrow$ block and path extraction step as well as the grouping step are often error-prone and fragile
\end{itemize}

\subsection*{Enhancement Strategies}
\subsubsection*{Feature Representation}
\begin{itemize}
    \item
        Idea: Downsample (with downsampling rate $d$) and smooth chroma features (with smoothing window of length $l$)
    \item
        Result: Emphasis on the rough harmonic content
\end{itemize}

\subsubsection*{Path Smoothing}
\begin{itemize}
    \item
        Idea: Apply averaging filter (or low-pass filter) in the direction of the main diagonal
    \item
        Result: Emphasis on diagonal information and a softening of other, non-diagonal structures\\
    
    \item
        Smoothing length $L$ and SSM $S$
    \item
        Smoothed self-similarity matrix $S_L = \ffrac{1}{L} \sum_{l=0}^{L-1} S(n+l, m+l)$
    \item
        Only works well if there are no relative tempo differences between the segments to be compared\\
    \item
        Idea: Apply a multiple filtering approach, where the SSM is smoothed along various directions that lie in a neighborhood of the direction defined by the main diagonal and then take cell-wise maximum over all these matrices
    \item
        Tempo difference given by real number $\theta > 0$ (e.g. second segment played $\theta$ times slower than the first one) gradient is (1, $\Theta$)
    \item
        $S_{L, \theta} (n,m) = \ffrac{1}{L}\sum_{l=0}^{L-1} S(n+l, m+[l\cdot \theta])$, where $[l\cdot \theta]$ is integer closest to the real number
    \item
        Consider finite set of tempo parameters $\Theta$ and calculate final matrix $S_{L, \Theta}(n,m) = \max_{\theta \in \Theta} S_{L, \theta}(n,m) $
\end{itemize}

\subsubsection*{Transposition Invariance}
\begin{itemize}
    \item
        Problem: certain musical parts get repeated in a transposed form, where the melody is moved up or down in pitch by a constant interval
    \item
        Cyclic shift operator $p: \mathbb{R}^{12} \rightarrow \mathbb{R}^{12}$
    \item
        \textbf{i-transposed self-similariy matrix} $p^i(S)(n,m) = s(p^i(x_n), x_m)$
    \item
        \textbf{transposition-invariant self-similarity matrix} $S^{TI}(n,m) = max_{i \in [0:11]}p^i(S)(n,m)$
    \item
        \textbf{transposition index matrix} $I(n,m) = argmax_{i \in [0:11]} p^i(S)(n,m)$
\end{itemize}

\subsubsection*{Thresholding}
\begin{itemize}
    \item
        Idea: Suppress values that fall below a given threshold in order to suppress noise
    \item
        Simplest strategy is \textbf{global thresholding} and/or binarization
\end{itemize}

\subsection*{Audio Thumbnailing}
\begin{itemize}
    \item
        Given a music recording, the objective is to automatically determine the most representative section
    \item
        Most procedures try to identify a section that has on the one hand a certain minimal duration and on the other many (approximate) repetitions
\end{itemize}

\subsubsection*{Fitness Measure}
\begin{itemize}
    \item
        Captures to aspects: how well a given segment explains other related segments, and how much of the overall music recording is covered by all the related segments
    \item
        Requirement: SSM S which fullfills property $S(n,m) \leq 1$ and $S(n,n) = 1$\\
    \item
        \textbf{Segment family} is a set $\mathcal{A}=\{\alpha_1, \dots, \alpha_K\}$ of pairwise disjoint semgents $\alpha_i \cap \alpha_j = \emptyset$
    \item
        Let $\gamma(\mathcal{A}) = \sum_{k=1}^{K} |\alpha_k|$ be the coverage of $\mathcal{A}$
    \item
        A \textbf{path family} over $\alpha$ is defined to be a set $\mathcal{P} = \{P_1, \dots, P_K\}$ consisting of paths $P_k$ over $\alpha$
    \item
        The set of induced segments $\{\pi(P_1), \dots, \pi(P_K)\}$ (\textbf{induced segment family}) is required to be a segment family (pairwise disjoint)
    \item
        The score of the path family is defined as $\sigma(\mathcal{P}) = \sum_{k=1}^K \sigma(P_k)$
    \item
        Optimal path family is $\mathcal{P}^* = argmax_{\mathcal{P}}\;\sigma(\mathcal{P})$\\
    \item
        Score of path family depends on size of $\alpha$ and the length of their paths and also captures trivial self-explanations
    \item
        Subtract length of self-explaining $\alpha$ from score (if S fullfils normalization properties)
    \item
        Divide by lengths of paths $L_k = |P_k|$
    \item
        Normalized score $\hat{\sigma}(\alpha) = \frac{\sigma(\mathcal{P}^*) - |\alpha|}{\sum_{k=1}^K L_k}$ (how well)
    \item
        Normalized coverage $\hat{\gamma}(\alpha) = \frac{\gamma(\mathcal{A}^*) - |\alpha|}{N}$ (how much)
    \item
        Fitness $\varphi(\alpha) = 2\cdot \ffrac{\hat{\gamma} \cdot \hat{\sigma}(\alpha)}{\hat{\gamma} + \hat{\sigma}(\alpha)}$ (Harmonic mean $\Rightarrow$ slightly favors shorter segments)
    \item
        Thumbnail $\alpha^* = argmax_{\alpha}\varphi(\alpha)$
\end{itemize}

\subsubsection*{Scape Plot Representation}
\begin{itemize}
    \item
        Each segment can be uniquely described by its center $c(\alpha) = (s+t)/2$ and its length $|\alpha|$
    \item
        Use center as horizontal and length as vertical coordinate
    \item
        Segments $\alpha'$ contained in a given segment $\alpha$ lie in subtriangle below $\alpha$
\end{itemize}

\subsection{Novelty-Based Segmentation}
\begin{itemize}
    \item
        Goal: locate points in time where musical changes occur, thus making the transition between two subsequent parts
    \item
        Idea: Correlate a checkerboard-kernel function along the main diagonal of the SSM, which yields a novelty function
    \item
        Simple $(2 \times 2)$-unit-kernel $K = \begin{bmatrix} -1 & 1 \\ 1 & -1 \end{bmatrix}$
    \item
        Kernel size is important (sudden vs. slow changes)
    \item
        Maxima of novelty curve correspond to changes in audio recording
    \item
        Only works if SSM has block-like structures
\end{itemize}

\subsection{Structure Features}
TODO
\end{document}
