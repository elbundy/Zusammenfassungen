\documentclass{scrartcl}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{gensymb}

\usepackage[a4paper, left=1cm, right=1cm, top=2cm, bottom=3cm]{geometry}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%
\DeclarePairedDelimiter\inner{\langle}{\rangle}%
\newcommand{\ffrac}[2]{\ensuremath{\frac{\displaystyle #1}{\displaystyle #2}}}

\begin{document} 
\tableofcontents
\newpage

\section{Fourier Transform in a Nutshell}
Key Idea: Compare signal with sinusoids of various frequencies and phases. 

\subsection{Continuous Fourier Transform}
\subsubsection*{Analog Signals}
\begin{itemize}
    \item
        Time as well as amplitude are continuous, real-valued parameters
    \item
        Can be modeled as a function $f: \mathbb{R} \rightarrow \mathbb{R}$, which assigns to each time point $t \in \mathbb{R}$ an amplitude value $f(t) \in \mathbb{R}$
\end{itemize}
\subsubsection*{Sinussoidal signal}
\begin{itemize}
    \item
        Function $g: \mathbb{R} \rightarrow \mathbb{R}$ defined by $g(t) = A sin(2\pi(\omega t - \varphi))$
    \item
        $A$ corresponds to the \textbf{amplitude}, $\omega$ to the \textbf{frequency} and $\varphi$ to the \textbf{phase}
    %\item
        %frequency is measured in Hz, phase in normalized radians (1 corresonds to an angle of 360\degree)
    \item
        In Fourier analysis: prototype oscillations that are normalized with regard to their power by setting $A = \sqrt2$
    \item
        Thus for each frequency parameter $\omega$ and phase parameter $\varphi$ we obtain a sinusoid 
        $$cos_{\omega, \varphi}(t) = \sqrt2 cos(2\pi(\omega t - \varphi))$$
    \item
        Phase parameter only has to be considered for $\varphi \in [0, 1)$
\end{itemize}
\subsubsection*{Computing Similarity with Integrals}
\begin{itemize}
    \item
        $\int_{t \in \mathbb{R}} f(t)g(t) dt$
\end{itemize}
\subsubsection*{First Definition of the Fourier Transform}
\begin{itemize}
    \item
        For a fixed frequency $\omega \in \mathbb{R}$, we define
        $$d_\omega = max_{\varphi \in [0,1)} (\int_{t \in \mathbb{R}} f(t) cos_{\omega, \varphi}(t) dt)$$
        $$\varphi_\omega = argmax_{\varphi \in [0,1)} (\int_{t \in \mathbb{R}} f(t) cos_{\omega, \varphi}(t) dt)$$
    \item
        \textbf{Fourier Transform:} "collection" of all coefficients $d_\omega$ and $\varphi_\omega$ for $\omega \in \mathbb{R}$
\end{itemize}
\subsubsection*{Complex Numbers}
\begin{itemize}
    \item
        Extends real numbers by introducing the imaginary number $i = \sqrt{-1}$ with $i^2 = -1$
    \item
        Gets written as $c = a + ib$ 
    \item
        Set of complex numbers $\mathbb{C}$ can be thought of as a two-dimensinal plane, where horizontal dimension corresponds to real part and vertical dimension to imaginary part (so $c = a+ib$ has Cartesian coordinates $(a,b)$)
    \item
        \textbf{Polar coordinates:} Complex number c is described by absolute value $\abs{c}$ (distance from origin) and angle $\gamma$ between the positive horizontal axis and the line from the origin to $c$
    \item
        Deriving polar coordinates:
        $$ \abs{x} = \sqrt{a^2 + b^2} $$
        $$ \gamma = atan2(b, a) $$
    \item
        Regaining complex number:
        $$ exp(i\gamma) = cos(\gamma) + isin(\gamma) $$
        $$ c = \abs{c} exp(i\gamma)$$
\end{itemize}
\subsubsection*{Complex Definition of the Fourier Transform}
\begin{itemize}
    \item
        Idea: encode $d_\omega$ and $\varphi_\omega$ by a single complex number
    \item
        $ c_\omega = \ffrac{d_\omega}{\sqrt{2}} exp(2\pi i(-\varphi_\omega))$
    \item
        $\hat{f}(\omega) = c_\omega$
    \item     
        $\hat{f}$ is referred to as the Fourier transform of f, $\hat{f}(\omega) = c_\omega$ are called the Fourier coefficients
    \item
        Fourier transform can be computed by

        \begin{align*}
            \hat{f}(\omega) = & \int_{t \in \mathbb{R}} f(t) exp(-2\pi i \omega t) dt\\
            & \int_{t \in \mathbb{R}} f(t) cos(-2\pi \omega t) dt + i \int_{t \in \mathbb{R}} f(t) sin(-2 \pi \omega t) dt
        \end{align*}
    \item
        Real part is obtained by comparing $f$ with a cosine function, the imaginary part by comparing $f$ with a sine function
    \item
        $\abs{\hat{f}(\omega)}$ is called the \textbf{magnitude Fourier transform}
    \item
        With $\abs{\hat{f}(\omega)}$ and $\gamma_\omega$ being the polar coordinates of $\hat{f}(\omega)$:
        $$d_\omega = \sqrt{2} \abs{\hat{f}(\omega)}$$
        $$\varphi_\omega = -\ffrac{\gamma_\omega}{2\pi}$$
\end{itemize}
\subsubsection*{Fourier Representation}
\begin{itemize}
    \item
        Signal reconstruction:
        \begin{align*}
            f(t) & = \int_{\omega \in \mathbb{R}_{\geq 0}} d_\omega \sqrt{2} cos(2 \pi (\omega t - \varphi_\omega))d\omega\\
            & = \int_{\omega \in \mathbb{R}} c_\omega exp(2\pi i \omega t)d \omega
        \end{align*}
        
\end{itemize}
\subsection{Discrete Fourier Transform}
\subsubsection*{Equidistant sampling}
\begin{itemize}
    \item
        Digitization: Converting analog signals into finite representations (analog-to-digital conversion) 
    \item
        Given an analog signal $f: \mathbb{R} \rightarrow \mathbb{R}$ and a positive real number $T > 0$, one defines a function $x: \mathbb{Z} \rightarrow \mathbb{Z}$ by setting $x(n) = f(nT)$
    \item
        Since $x$ is only defined on a discrete set of time points, it is also refered to as a \textbf{discrete-time} (DT) signal
    \item
        $x(n)$ is called a sample, taken at time $t = nT$
    \item
        $F_s = 1/T$ is called sampling rate
    \item
        \textbf{Sampling theorem:} Original signal $f$ can be reconstructed perfectly from its sampled version $x$, if $f$ does not contain any frequencies higher than
        $$ \Omega = F_s/2 = 1/(2T) Hz $$
    \item
        We say $f$ is an \textbf{$\Omega$-bandlimited} signal, $\Omega$ is known as the \textbf{Nyquist frequenct}
    \item
        In the case $f$ contains higher frequencies, sampling may cause artifacts referred to as \textbf{aliasing}
\end{itemize}
\subsubsection*{Discrete Fourier Transform}
\begin{itemize}
    \item
        Sample the sinusoudal prototype oscillation in the same fashion as the signal and multiply the two sampled functions in a pointwise fashion (sampled product)
    \item
        Integration in the continuos case becomes summation in the discrete case, where the summands need to be weighted by the sampling period $T$
        $$\sum_{n \in \mathbb{Z}} T f(nT) exp(-2\pi i \omega n T) \approx \hat{f}(\omega)$$
    \item
        "Approximation of integral area via sum of rectangular shapes" (\textbf{Riemann sum})
    \item
        Substituting $T = 1$ and $f(nT) = x(n)$ yields
        $$ \hat{x}(\omega) = \sum_{n \in \mathbb{Z}} x(n) exp(-2\pi i \omega n) $$
    \item
        To recover relation to Fourier transform $\hat{f}$, one needs to know the sampling period $T$
        $$ \hat{x}(\omega) \approx \ffrac{1}{T} \hat{f}(\ffrac{\omega}{T})$$
    \item
        $\omega = 1/2$ for $\hat{x}$ corresponds to the Nyquist frequency $\Omega = 1/(2T)$ of the sampling process
    \item
        Two \textbf{problems} for computation: 
        \begin{enumerate}
            \item
                Infinite number of summands ($\sum_{n \in \mathbb{Z}}$)
            \item
                $\omega$ is a continuos parameter
        \end{enumerate}
    \item
        \textbf{Solutions:}
        \begin{enumerate}
            \item
                Assumption: most relevant information of $f$ is limited to a certain duration in time (e.g. song only lasts a few minutes). This means that we only need to consider a finite amount of samples $x(0),\dots,x(N-1)$ and the sum becomes finite.
            \item
                One computes the Fourier transform only for a finite number of frequencies ("sampling frequency axis" by considering frequencies $\omega = k/M$ for some suitable $M \in \mathbb{N}$ and $k \in [0: M-1]$). Often $N=M$ gets choosen.
        \end{enumerate}
    \item
        \textbf{Discrete Fourier Transform:}
        $$ X(k) = \hat{x}(k/N) = \sum_{n=0}^{N-1} x(n) exp(-2\pi i k n/N)$$
        defined for integers $k \in [0: M-1] = [0 : N-1]$
    \item
        The frequency $\omega$ of $\hat{x}$ corresponds to $\omega/T$ of $\hat{f}$, so $F_{coef}(k) = \ffrac{k}{N \cdot T} = \ffrac{k \cdot F_s}{N}$
    \item
        Upper half of coefficients are redundant, one only needs to consider the coefficients $X(k)$ for $k \in [0: \lfloor N/2 \rfloor]$
    \item
        To compute all coefficients, one needs $N^2$ operations
    \item
        \textbf{Fast Fourier Transform}: Recursive computation, works particularly well in the case of N is a power of two, $Nlog_2N$ operations
\end{itemize}

\subsection{Short-Time Fourier Transform}
\begin{itemize}
    \item
        Instead of considering the entire signal, the main idea of the STFT is to consider only a small section of the signal 
    \item
        One fixes a so-called \textbf{window function}, which is a function that is nonzero for only a short period of time
    \item
        The original signal is multiplied with the window funtion to yield a \textbf{windowed signal}
    \item
        To obtain frequency information at different time instances, one shifts the window function across time and computes a FT for each of the resulting windows
    \item
        STFT reflects not only the properties of original signal but also those of the window function (e.g. rectangular windows typically introduce "ripple" artifacts)
    \item
        Parameters:
        \begin{itemize}
            \item
                Real-valued DT signal $x: \mathbb{Z} \rightarrow \mathbb{R}$ obtained with sampling rate $F_s$
            \item
                Sampled window function $w: [0:N-1] \rightarrow \mathbb{R}$ of length $N \in \mathbb{N}$
            \item
                Hop size $H \in \mathbb{N}$ specified in samples and determining the step size in which the window is to be shifted across the signal (most of the time $H = N/2$)
        \end{itemize}
    \item
        \textbf{Discrete STFT $X$:}
        $$X(m,k) = \sum_{n=0}^{N-1} x(n+mH)w(n)exp(-2\pi i k n/N)$$
        with $m \in \mathbb{Z}$ and $k \in [0:K]$
    \item
        The number $K = N/2$ is the frequency index corresponding to the Nyquist frequency
    \item
        $X(m,k)$ denotes the $k^{th}$ Fourier coefficient for the $m^{th}$ time frame
    \item
        For each fixed time frame $m$, one obtains a spectral vector of size $K+1$. The computation of each such spectral vector amounts to a DFT of size $N$, which can be done efficiently using the FFT
    \item
        $T_{coef}(m) = \ffrac{m \cdot H}{F_s}$ (in seconds) and $F_{coef}(k) = \ffrac{k \cdot F_s}{N}$ (in Hertz)
    \item
        \textbf{Spectrogram}: two-dimensional representation of the squared magnitude of the STFT
        $$Y(m,k) = \abs{X(m,k)}^2$$
\end{itemize}

\newpage

\section{Music Synchronization}
\begin{itemize}
    \item
        Task: For a given position in one representation of a piece of music, determine the corresponding position within another representation (audio, midi or sheet music in general)
    \item
        Two main steps:
        \begin{enumerate}
            \item
                Transform the two representations into sequence of suitable feature vectors\\
                (large degree of robustness to variations that are to be left unconsidered for the task at hand vs. capture enough characteristic information to accomplish the given task) $\rightarrow$ chroma-based features
            \item
                Bring derived feature sequences into temporal correspondence
        \end{enumerate}
\end{itemize}
\subsection*{Dynamic Time Warping}
\begin{itemize}
    \item
        Two chroma vector sequences $X = (x_1, x_2, \dots, x_N)$ and $Y = (y_1, y_2, \dots, y_M)$
    \item
        Goal: find non-linear alignment between the elements of the two sequences in order to compensate differences in tempo
    \item
        Can be achieved by skipping certain elements of a sequence or by using ceratin elements more than once
\end{itemize}
\subsubsection*{Basic Approach}
\begin{itemize}
    \item
        local cost measure or distance measure $c:\mathcal{F} \times \mathcal{F} \rightarrow \mathbb{R}$     
        \begin{itemize}
            \item
                Euclidian distance: $\norm{x - y}$
            \item
                Cosine distance: $1 - \ffrac{\inner{x, y}}{\norm{x} \cdot \norm{y}}$\\
                Inner product corresponds to cosine of angle between vectors $x$ and $y$\\
                Comparison only considers the energy distributions across the twelve chroma bands and disregards actual local energy\\
                Can be computed efficiently using a simple matrix multiplication
        \end{itemize}
    \item
        Evaluating the local cost measure for each par of elements in $X$ and $Y$, one obtains a cost matrix $C \in \mathbb{R}^{N \times M}$ defined by
        $C(n,m) = c(x_n, y_m)$
    \item
        Goal: find 'valley' of low cost within the cost matrix $C$
\end{itemize}
\subsubsection*{Warping Path}
\begin{itemize}
    \item
        $(N,M)$-warping path of length $L$ is a sequence $P=(p_1, ..., p_L)$ with $p_l = (n_l, m_l)$ satisfying the following three conditions:
        \begin{enumerate}
            \item
                Boundary condition: $p_1 = (1,1)$ and $p_L = (N,M)$
            \item
                Monotonicity condition: $n_1 \leq n_2 \leq \dots \leq n_L$ and $m_1 \leq m_2 \leq \dots \leq m_L$
            \item
                Step size condition: $p_{l+1} - p_l \in \{(1,0), (0,1), (1,1)\}$
        \end{enumerate}
        (3. implies 2.)
    \item
        Warping path assigns element $x_{n_l}$ to element $y_{n_l}$
\end{itemize}
\subsection*{Optimal Warping Path and DTW Distance}
\begin{itemize}
    \item 
        total cost $c_P(X,Y)$ of a warping path $P$ between two sequences $X$ and $Y$ with respects to the local cost measure $c$ is defined as
        $$c_P(X,Y) = \sum_{l=1}^{L}c(x_{n_l}, y_{m_l}) = \sum_{l=1}^{L} C(n_l, m_l)$$
    \item
        DTW distance is defined as the total cost of an optimal $(N,M)$-warping path $P^*$:
        $$DTW(X,Y) = c_{P^*}(X,Y) = min\{c_P(X,Y)|\text{P is an (N,M)-warping path}\}$$
    \item
        there may be multiple optimal warping paths, e.g. when cost matrix is zero everywhere
\end{itemize}

\subsection*{Dynamic Programming Algorithm}
\begin{itemize}
    \item
        Naive approach: compute all possible warping paths and then take minimal cost one
    \item
        Problem: Number of different $(N,M)$-warping paths is exponential in $N$ and $M$
    \item
        Better: Dynamic Programming with $O(NM)$ complexity
    \item
        Basic idea is to break down problem into simpler subproblems and then to combine the solutions
    \item
        Here this means to find optimal warping paths for truncated subsequences of $X$ and $Y$ \\
    \item
        $X(1:n) = (x_1, \dots, x_n)$, $Y(1:m) = (y_1, \dots, y_m)$ and $D(n,m) = DTW(X(1:n), Y(1:m))$
    \item
        Matrix $D$ is called accumulated cost matrix
    \item
        Recursive computation:
        $$D(n,1) = \sum_{k=1}^n C(k,1) \text{for } n \in [1:N]$$
        $$D(1,m) = \sum_{k=1}^m C(1,k) \text{for } n \in [1:N]$$
        $$D(n,m) = C(n,m) + min \{D(n-1, m-1), D(n-1,m), D(n, m-1)\}$$
        for $n\in[2:N]$ and $m\in[2:M]$\\
        Proof: See book
    \item
        Computing $D$ has complexity $O(NM)$
    \item
        Finding optimal warping path: Start with $q_1=(N,M)$ and continue following until $q_L = (1,1)$ reached:
        \begin{itemize}
            \item
                $q_{l+1} = (1, m-1) \text{if } n=1$
            \item
                $q_{l+1} = (n-1, 1) \text{if } n=1$
            \item
                $q_{l+1} = argmin\{D(n-1,m-1), D(n-1, m), D(n,m-1)\}$
        \end{itemize}
    \item
        Problem: still infeasible for large $N$ and $M$
    \item
        Better: Global constraints (Limit region where optimal warping path can be), Multiscale approach
\end{itemize}

\newpage

\section{Music Structure Analysis}
\begin{itemize}
    \item
        Musical parts of popular music: intro, chorus, verse, ...
    \item
        Goal: divide a given music representation into temporal segments that correspond to musical parts and to group these segments into musically meaningful categories (find and understand the relationship between the segments)
    \item
        Musical structure is related to:
        \begin{itemize}
            \item
                Repetition, e.g. recurring patterns of rythmic, harmonic or melodic nature
            \item
                Homogenity, e.g. consistent timber, presence of specific instrument or usage of certain harmonies
            \item
                Novelty (Neuheit): e.g. segment boundaries go along with sudden changes in musical properties such as tempo, dynamics or the musical key
        \end{itemize}
        $\rightarrow$ \textbf{repetition-based}, \textbf{novelty-based} and \textbf{homogenity-based} methods
    \item
        First step: transform the given music recording into a suitable feature representation (trade-off between robustness and expressiveness)
    \item
        Feature types:
        \begin{itemize}
            \item
                chroma features: harmonic and melodic properties
            \item
                mel-frequency cepstral coefficients (MFCCs): instrumentation and timber
            \item
                beat-synchronous features, tempogram: beat and tempo (and rythm)
        \end{itemize}
\end{itemize}
\subsection*{Self-Similarity Matrices}
\begin{itemize}
    \item
        Idea: Convert musical recording into sequence of feature vectors and compare each element with all other elements of the sequence
    \item
        Result: Self-Similarity Matrix (SSM)
    \item
        One crucial property of SSMs is that repetitions typically yield path-like structures, whereas homogeneous regions yield block-like structures\\
    \item
        Similarity measure $s:\mathcal{F} \times \mathcal{F} \rightarrow \mathbb{R}$
        \begin{itemize}
            \item
                simple measure is absolute value of inner product: $s(x,y) = |\inner{x,y}|$
            \item
                if feature vectors are normalized with respects to the Euclidian norm, $s(x,y) \in [0,1]$
        \end{itemize}
    \item
        N-square \textbf{self-similarity matrix} $S\in \mathbb{R}^{N\times N}$ defined by $S(n,m) = s(x_n, x_m)$
    \item
        Diagonal with high values
    \item
        Segment is defined as $\alpha = [s:t]$ with $|\alpha| = t-s+1$ being the length of the segment
    \item
        \textbf{Path} over $\alpha$ of length L is a sequence $P=((n_1, m_1), \dots, (n_L, m_L))$ with
        \begin{itemize}
            \item
                $m_1 = s, m_L=t$ (boundary condition)
            \item
                $(n_{l+1}, m_{l+1}) - (n_l, m_l) \in \Sigma$, $\Sigma = \{(2,1), (1,2), (1,1)\}$
        \end{itemize}
    \item
        For a path one can associate two segments defined by the projections\\
        $\pi_1(P) = [n_1 : n_L]$ and $\pi_2(P) = [m_1 : m_L]$\\
        where the boundary condition enforces $\pi_2(P)=\alpha$. $\pi_1(P)$ is refered to as the \textbf{induced segment}
    \item
        The score $\sigma(P)$ is defined as $\sigma(P) = \sum_{l=1}^{L} S(n_l, m_l)$ (yields measure of similarity between $\alpha$ and the induced segment)
    \item
        \textbf{Block} over $\alpha$ is a subset $B = \alpha' \times \alpha \subseteq [1:N] \times [1:N]$ for some segment $\alpha' = [s', t']$ (N is length of feature sequence)
    \item
        Similarily we define two projections $\pi_1(B) = \alpha'$ and $\pi_2(B) = \alpha$ , where $\alpha'$ is the induced segment
    \item
        Based on paths and blocks we can now consider differnt kinds of similarity relations between segments
    \item
        Segment $\alpha_1$ is \textbf{path-similar} to a segment $\alpha_2$, if there is a path $P$ of high score with $\pi_1(P) = \alpha_1$ and $\pi_2(P) = \alpha_2$
    \item
        Segement $\alpha_1$ is \textbf{block-similar} to $\alpha_2$, if there is a block B of high score with $\pi_1(B) = \alpha_1$ and $\pi_2(B) = \alpha_2$
    \item
        If similarity measure is symmetric, then SSM S and as well as similarity relations are symmetric
    \item
        Transitivity of similarity relation: $\alpha_1$ similar to $\alpha_2$ and $\alpha_2$ similar to $\alpha_4$ $\Rightarrow$ $\alpha_1$ similar to $\alpha_3$ (at least to a certain degree)
    \item
        Consequence: Block-similarity between $\alpha$ and $\alpha'$, and $\alpha$ and $\alpha'$ $\Rightarrow$ Block-similarity between $\alpha$ and $\alpha$\\
    \item
        Algorithmic pipeline:
        \begin{enumerate}
            \item
                The musical signal is transformed into a suitable feature sequence
            \item
                A self-similarity matrix is computed from the feature sequence based on a similarity measure
            \item
                Blocks and paths of high overall score are derived from the SSM. Each block or path defines a pair of similar segments
            \item
                Entire groups of mutually similar segments are formed from the pairwise relations by applying a clustering step
        \end{enumerate}
    \item
        Leaves a lot of freedom and needs to be adjusted to account for particular properties of the underlying type of music and the requirements of the intended application
    \item
        Major challenges arise from the fact that musical parts are rarely repeated in precisely the same way (differ in dynamics, orchestration, articulation, tempo, harmony, melody or any combination of these) $\rightarrow$ block and path extraction step as well as the grouping step are often error-prone and fragile
\end{itemize}

\subsection*{Enhancement Strategies}
\subsubsection*{Feature Representation}
\begin{itemize}
    \item
        Idea: Downsample (with downsampling rate $d$) and smooth chroma features (with smoothing window of length $l$)
    \item
        Result: Emphasis on the rough harmonic content
\end{itemize}

\subsubsection*{Path Smoothing}
\begin{itemize}
    \item
        Idea: Apply averaging filter (or low-pass filter) in the direction of the main diagonal
    \item
        Result: Emphasis on diagonal information and a softening of other, non-diagonal structures\\
    
    \item
        Smoothing length $L$ and SSM $S$
    \item
        Smoothed self-similarity matrix $S_L = \ffrac{1}{L} \sum_{l=0}^{L-1} S(n+l, m+l)$
    \item
        Only works well if there are no relative tempo differences between the segments to be compared\\
    \item
        Idea: Apply a multiple filtering approach, where the SSM is smoothed along various directions that lie in a neighborhood of the direction defined by the main diagonal and then take cell-wise maximum over all these matrices
    \item
        Tempo difference given by real number $\theta > 0$ (e.g. second segment played $\theta$ times slower than the first one) gradient is (1, $\Theta$)
    \item
        $S_{L, \theta} (n,m) = \ffrac{1}{L}\sum_{l=0}^{L-1} S(n+l, m+[l\cdot \theta])$, where $[l\cdot \theta]$ is integer closest to the real number
    \item
        Consider finite set of tempo parameters $\Theta$ and calculate final matrix $S_{L, \Theta}(n,m) = \max_{\theta \in \Theta} S_{L, \theta}(n,m) $
\end{itemize}

\subsubsection*{Transposition Invariance}
\begin{itemize}
    \item
        Problem: certain musical parts get repeated in a transposed form, where the melody is moved up or down in pitch by a constant interval
    \item
        Cyclic shift operator $p: \mathbb{R}^{12} \rightarrow \mathbb{R}^{12}$
    \item
        \textbf{i-transposed self-similariy matrix} $p^i(S)(n,m) = s(p^i(x_n), x_m)$
    \item
        \textbf{transposition-invariant self-similarity matrix} $S^{TI}(n,m) = max_{i \in [0:11]}p^i(S)(n,m)$
    \item
        \textbf{transposition index matrix} $I(n,m) = argmax_{i \in [0:11]} p^i(S)(n,m)$
\end{itemize}

\subsubsection*{Thresholding}
\begin{itemize}
    \item
        Idea: Suppress values that fall below a given threshold in order to suppress noise
    \item
        Simplest strategy is \textbf{global thresholding} and/or binarization
\end{itemize}

\subsection*{Audio Thumbnailing}
\begin{itemize}
    \item
        Given a music recording, the objective is to automatically determine the most representative section
    \item
        Most procedures try to identify a section that has on the one hand a certain minimal duration and on the other many (approximate) repetitions
\end{itemize}

\subsubsection*{Fitness Measure}
\begin{itemize}
    \item
        Captures to aspects: how well a given segment explains other related segments, and how much of the overall music recording is covered by all the related segments
    \item
        Requirement: SSM S which fullfills property $S(n,m) \leq 1$ and $S(n,n) = 1$\\
    \item
        \textbf{Segment family} is a set $\mathcal{A}=\{\alpha_1, \dots, \alpha_K\}$ of pairwise disjoint semgents $\alpha_i \cap \alpha_j = \emptyset$
    \item
        Let $\gamma(\mathcal{A}) = \sum_{k=1}^{K} |\alpha_k|$ be the coverage of $\mathcal{A}$
    \item
        A \textbf{path family} over $\alpha$ is defined to be a set $\mathcal{P} = \{P_1, \dots, P_K\}$ consisting of paths $P_k$ over $\alpha$
    \item
        The set of induced segments $\{\pi(P_1), \dots, \pi(P_K)\}$ (\textbf{induced segment family}) is required to be a segment family (pairwise disjoint)
    \item
        The score of the path family is defined as $\sigma(\mathcal{P}) = \sum_{k=1}^K \sigma(P_k)$
    \item
        Optimal path family is $\mathcal{P}^* = argmax_{\mathcal{P}}\;\sigma(\mathcal{P})$\\
    \item
        Score of path family depends on size of $\alpha$ and the length of their paths and also captures trivial self-explanations
    \item
        Subtract length of self-explaining $\alpha$ from score (if S fullfils normalization properties)
    \item
        Divide by lengths of paths $L_k = |P_k|$
    \item
        Normalized score $\hat{\sigma}(\alpha) = \frac{\sigma(\mathcal{P}^*) - |\alpha|}{\sum_{k=1}^K L_k}$ (how well)
    \item
        Normalized coverage $\hat{\gamma}(\alpha) = \frac{\gamma(\mathcal{A}^*) - |\alpha|}{N}$ (how much)
    \item
        Fitness $\varphi(\alpha) = 2\cdot \ffrac{\hat{\gamma} \cdot \hat{\sigma}(\alpha)}{\hat{\gamma} + \hat{\sigma}(\alpha)}$ (Harmonic mean $\Rightarrow$ slightly favors shorter segments)
    \item
        Thumbnail $\alpha^* = argmax_{\alpha}\varphi(\alpha)$
\end{itemize}

\subsubsection*{Scape Plot Representation}
\begin{itemize}
    \item
        Each segment can be uniquely described by its center $c(\alpha) = (s+t)/2$ and its length $|\alpha|$
    \item
        Use center as horizontal and length as vertical coordinate
    \item
        Segments $\alpha'$ contained in a given segment $\alpha$ lie in subtriangle below $\alpha$
\end{itemize}

\subsection{Novelty-Based Segmentation}
\begin{itemize}
    \item
        Goal: locate points in time where musical changes occur, thus making the transition between two subsequent parts
    \item
        Idea: Correlate a checkerboard-kernel function along the main diagonal of the SSM, which yields a novelty function
    \item
        Simple $(2 \times 2)$-unit-kernel $K = \begin{bmatrix} -1 & 1 \\ 1 & -1 \end{bmatrix}$
    \item
        Kernel size is important (sudden vs. slow changes)
    \item
        Maxima of novelty curve correspond to changes in audio recording
    \item
        Only works if SSM has block-like structures
\end{itemize}

\subsection{Structure Features}
TODO

\newpage

\section{Tempo and Beat Tracking}
\begin{itemize}
    \item
	Beat: sequence of perceived pulse positions, which are typically equally space in time and specified by the parameters phase and period
    \item
	Tempo: Reciprocal of beat period
    \item
	Common first step: estimate the positions of note onsets within the music signal (onset detection)
\end{itemize}
\subsection*{Onset Detection}
\begin{itemize}
    \item
	All approaches have common pipeline:
	\begin{enumerate}
	    \item
		Convert signal into a suitable feature representation
	    \item
		Application of derivative operator onto feature sequence which yields a novelty function
	    \item
		Peak-picking algorithm
	\end{enumerate}
\end{itemize}
\subsubsection*{Energy-Based Novelty}
\begin{itemize}
    \item
	Playing note often coincides with a sudden increase of the signals energy
    \item
	\begin{enumerate}
	    \item
		Amplitude squaring
	    \item
		Windowing
	    \item
		Differentiation
	    \item
		Half wave rectification
	    \item
		Peak picking
	\end{enumerate}
    \item
	It can be beneficial to switch to logarithmic decibel scale (but may also amplify noise-like sound components)
\end{itemize}
\subsubsection*{Spectral-Based Novelty}
\begin{itemize}
    \item
	$\mathcal{X}$: discrete STFT of the DT-signal $x$
    \item
	To detect spectral changes in the signal, one basically computes the difference between subsequent spectral vectors using a suitable distance measure $\rightarrow$ spectral-based novelty function or spectral flux
    \item
	Typical procedure:
	\begin{enumerate}
	    \item
		To enhance weak spectral components, we apply logarithmic compression which yields\\
		$\mathcal{Y} = \Gamma(|\mathcal{X}|) = log(1+\gamma \cdot |\mathcal{X}|)$
	    \item
		Compute discrete temporal derivative of the compressed spectrogram\\
		$\Delta_{Spectral(n)} = \sum_{k=0}^{K}|\mathcal{Y}(n+1,k)-\mathcal{Y}(n,k)|_{\geq 0}$
	    \item
		Accumulate intensity changes per frame, which yields a novelty curve
	    \item
		Possible enhancement step: Apply averaging window onto signal and subtract from original (and do half wave rectification) (Subtraction of local averages, Normalization)
	    \item
		Peak picking
	\end{enumerate}
    \item
	Multiband approach (also for energy-based novelty): Split signal into several frequency bands (e.g. 5-8), calculate novelty functions, weight and sum up
    \item
	Usually many spurious peaks $\rightarrow$ local thresholding
\end{itemize}

\subsection*{Tempo Analysis}
\begin{itemize}
    \item
	Most automated approaches rely on two basic assumptions:
	\begin{enumerate}
	    \item
		Beat positions occur at note onset positions
	    \item
		Beat positions are more or less equally spaced, at least for a certain period of time
	\end{enumerate}
\end{itemize}
\subsubsection*{Tempogram Representation}
\begin{itemize}
    \item
	Indicates for each time instance the local relevance of a specific tempo for a given music recording
    \item
	$\mathcal{T}(t, \tau)$ indicated the extent to which the signal contains a locally periodic pulse of a given tempo $\tau$ in a neighboorhood of time instances $t$
    \item
	Two steps:
	\begin{enumerate}
	    \item
		Convert music signal into a novelty function
	    \item
		Analyze locally periodic behavior of novelty function
	\end{enumerate}
    \item
	$\omega = 1/T$ and $\tau = 60 \cdot \omega$
    \item
	Harmonics of $\tau: \tau, 2\tau, 3\tau, ...$
    \item
	Subharmonics: $\tau, \tau /2, \tau /3, ...$
    \item
	Difference of two tempi with half or double the value is called tempo octave
\end{itemize}

\subsubsection*{Fourier Tempogram}
\begin{itemize}
    \item
	$\mathcal{F}(n,\omega) = \sum_{m\in \mathbb{Z}} \Delta(m) \overline{w}(m - n) exp(-2\pi i \omega m)$
    \item
	Hopsize equal $H=1$
    \item
	Comparing signal with windowed sinusoid of frequency $\omega$
    \item
	$\mathcal{T}^F (n,\tau) = |\mathcal{F}(n,\tau /60)|$
    \item
	Fourier tempogram generally indicates tempo harmonicsl, but supresses tempo subharmonics
\end{itemize}

\subsubsection*{Autocorrelation Tempogram}
\begin{itemize}
    \item
	Autocorrelation is a mathmatical tool for measuring the similarity of a signal with a time-shifted version of itself (also known as sliding inner product)
    \item
	Autocorellation: $R_{xx}(l) = \sum_{m \in \mathbb{Z}} x(m)x(m-l)$
    \item
	Maximal for $l=0$ and function that is symmetric in $l$
    \item
	Windowed version of novelty function: $\Delta_{w,n} (m) = \Delta (m) w(m-n)$
    \item
	Short-time auto corellation: $\mathcal{A}(n,l) = \sum_{m \in \mathbb{Z} \Delta (m) w(m-n) \Delta (m-l) w(m-n-l}$
    \item
	Zero for all but a finite number of time lag parameters
    \item
	Indicated tempo subharmonics, not harmonics
    \item
	In order to estimate global tempo value of recording: Average over time and take maximum
\end{itemize}

\subsection*{Beat and Pulse Tracking}
\begin{itemize}
    \item
	Idea:
	\begin{enumerate}
	    \item
		For each time position of Fourier tempogram compute maximizing tempo parameter and it's phase
	    \item
		Sum up all windowed sinusoids with correct phase and frequency of maximizing tempo parameter(overlap-add)
	    \item
		Half wave rectify
	    \item
		This yields PLP (predominant local pulse) function

	\end{enumerate}
    \item
	PLP refers to strongest pulse level that is measurable in the underlying novelty function
    \item
	Local periodicity enhancement of the original novelty function
    \item
	Todo: Discussion of Properties
\end{itemize}

\newpage

\section{Content-Based Audio Retrival}
\begin{itemize}
    \item
        Audio Identification: Given a small audio fragment as query, find the audio recording that is the source of the query
    \item
        Audio Matching: Given query, find audio excerpts that musically correspond to query
    \item
        Version Identification: Find remixes, cover songs, ...

\end{itemize}
\subsection*{Audio Identifikation}
\begin{itemize}
    \item
        The audio material is compared by means of so-called \textbf{audio fingerprints}, which are compact and descriptive audio features
    \item
        Requirements:
        \begin{itemize}
            \item
                High specifity
            \item
                Robustness (noise, distortion, ...)
            \item
                Compactness
            \item
                Scalability
        \end{itemize}
        Trade-off between requirements, improving one often implies losing perfomance in some other
\end{itemize}
\subsubsection*{Audio Fingerprints Based on Spectral Peaks}
\textbf{Design of Audio Fingerprints}
\begin{itemize}
    \item
        First: Calculate STFT from audio signal
    \item
        Second: Apply peak-piking strategy that identifies time-frequency points that have a higher magnitude than all their neighbors within a region around the point (reduces complicated spectrogram representation into sparse set of coordinates (\textbf{constellation map}))
\end{itemize}
\textbf{Fingerprint Matching}
\begin{itemize}
    \item
        Let $C(Q)$ and $C(D)$ be the constellation maps of the query $Q$ and the document $D$
    \item
        Shifting the query by $m \in \mathbb{Z}$ positions yields the constellation map
        $$ m + C(Q) = \{(m + n, k | (n,k) \in C(Q))\}$$
    \item
        Matching function: $\Delta_C(m) = |(m + C(Q)) \land C(m)|$
\end{itemize}
\textbf{Indexing, Retrieval, Inverted Lists}
\begin{itemize}
    \item
        Query point $(n,h) \in F(Q)$, where $n$ is time stamp and $h$ is hash
    \item
        Inverted list $L(h)$ consists (for fixed $h$) of time stamps $n$ with $(n, h) \in F(D)$
    \item
        $L(h) - n = \{ l - n | l \in L(h)\}$ contains all the shifts that, when applied to the query point $(n,h)$ result in a match with a database point
    \item
        Matching function: $\Delta_F (m) = \sum_{(n,h) \in F(Q)} 1_{L(h) - n}(m)$ where $ 1_B(a) = 1$ when $a \in B$\\
    \item
        Let $N = |F(D)|$, $M = |F(Q)|$ and $L = |H|$
    \item
        Assuming hash values of database are evenly distributed, each of the $L$ inverted lists contains $N/L$ elements
    \item
        Index can be computed offline
    \item
        In the query stage, only the information contained in the inverted lists corresponding to the $M$ items of $F(Q)$ is needed
    \item
        The $M$ lists need to be accessed, shifted and futher processed
    \item
        Overall complexity: Access and process $M$ lists of length $N/L$ $\rightarrow$ linear in $\ffrac{M\cdot N}{L}$
    \item
        Naive approach: linear in $M \dot N$, gain of factor $L$ 
    \item
        Idea: Further increase number $L$ of hashes
\end{itemize}
\textbf{Indexing (Shazam)}
\begin{itemize}
    \item
        Fix a anchor point and define a target zone
    \item
        Use every point as anchor point
    \item
        Hash: $(f_1, f_2, \Delta t)$\\
        $\rightarrow$ Increased specifity of hashes $\rightarrow$ much smaller hash lists $\rightarrow$ profit!
\end{itemize}
...

\newpage

\section{Chord Recognition}
\begin{itemize}
    \item
        Goal: Given a audio recording of a piece of music, find which chords are played at which time
\end{itemize}
\subsection*{Template-Based Chord Recognition}
\begin{itemize}
    \item
        First step: transform recording into a sequence $X = (x_1, x_2, \dots, x_N)$ of feature vectors
    \item
        Second step: 

        \begin{itemize}
            \item 
                map feature vectors to a chord label via pattern matching
            \item
                Precompute a set of templates (prototypical chroma vectors the represents a specific mucial chord)
            \item
                Similarity measure
        \end{itemize}
\end{itemize}
\subsubsection*{Ambiguities}
\begin{itemize}
    \item
        Chord Ambiguities: Chords may share same notes
    \item
        Acoustic Ambiguities: Harmonics, Major-Minor confusion 
    \item
        Tuning: Instrument not tuned as specified by the center frequency of the equal-tempered scale ($\rightarrow$ notes energy gets spread across neighboring chroma band)
    \item
        Segmentation: Broken chords
\end{itemize}
\subsubsection*{Enhancement Strategies}
\begin{itemize}
    \item
        Templates with harmonics (e.g. energy of $k^{th}$ harmonic is $\alpha^{k-1}$ for some $\alpha \in [0,1]$)
    \item
        Templates from examples (e.g. take average chroma vectors), naturally inherit the musical and acoustic properties. It sufficies to learn $C$ and $Cm$ chord and apply cyclic shifting
    \item
        Spectral enhancement, "smoothing" (e.g. Logarithmic compression)
    \item
        Temporal smoothing, Prefiltering (e.g. shift averaging filter over chroma feature sequence)
\end{itemize}
\subsection*{HMM-Based Chord Recognition}
\begin{itemize}
    \item
        Template-based chord recognizer only considers current frame, not frame sequence
    \item
        Some harmonic progressions are much more likeli then other, e.g. $I-IV-V-I$
\end{itemize}
\subsubsection*{Markov Chains and Transition Probabilities}
\begin{itemize}
    \item
        Considered chords are refered to as \textbf{states}, $\mathcal{A} = \{\alpha_1, \alpha_2, \dots, \alpha_I\}$
    \item
        The change from one state to another is specified according to set of probabilities (assumption: probability only depends on current state, not the ones before, "memoryless"), \textbf{state transition probabilities}
    \item 
        \textbf{Initial state probabilities}
\end{itemize}
\subsection*{Hidden Markov Models}
\begin{itemize}
    \item
        With Markov chain we can compute probability for a given observation consisting of a sequence of chords
    \item
        Chord recognition task however we observe a sequence of chroma vectors
    \item
        Each state is equipped with a probability function that expresses the likelihood for a given chord type to output or emit a certain feature vector (\textbf{emission, output probabilities})
    \item
        Discrete HMM: Output space is assumed to be discrete and finite $\rightarrow$ Finite set of output elements $\mathcal{B} = \{\beta_1, \dots, \beta_2\}$
    \item
        Summary: HMM is fully specified by five tuple\\ (set of states, state trans. probs., init state probs. observation syms., emission probs)
\end{itemize}
\subsection*{Evaluation and Model Specification}
\textbf{Evaluation problem}
\begin{itemize}
    \item
        Given a observation sequence $O$, compute probability $P(O|\Theta)$
    \item
        "Score" of how well a given model matches a given observation sequence, interesting when one has to choose among several competing models
    \item
        $P(O|\Theta) = \sum_{S=(s_1, s_2, s_N)} P(0, S|\Theta)$
\end{itemize}
\textbf{Uncovering Problem}
\begin{itemize}
    \item
        Find state sequence that best explains observation sequence 
    \item
        Possible optimization criterion:
        $S^* = \underset{S = (s_1, s_2, \dots, s_N)}{argmax} P(O,S|\Theta)$
    \item
        Viterbi Algorithm:
        \begin{itemize}
            \item
                Observation sequence $O=(o_1, \dots, o_N)$
            \item
                $O(1:n) = (o_1, \dots, o_n)$
            \item
                $D(i:n) = \underset{S}{max} \; P(O(1:n), (s_1, \dots, s_n=\alpha_i)|\Theta)$, highest probability of $O(1:n)$ with state sequence ending on $\alpha_i$
            \item
                $(I \times N)$ matrix $D$ can be computed recursively\\
            \item
                $ S^* = (\alpha_i, \dots, \alpha_{i_N})$
            \item
                Apply backtracking procedure
            \item
                $i_N = \underset{j \in [1:I]}{argmax} D(j, N)$, ending state of state sequence, which is most probable to generate observations
            \item
                $i_n = \underset{j \in [1:I]}{argmax} \; \; a_{j, i_{n+1}}D(j, n)$, ending state of state sequence of length $n$, which is most probable to create observations $O(1:n)$ weighted with transition probability to previously calculated state $i_{n+1}$\\
            \item
                $O(I^2 N)$ operations much better than brute force $O(I^N)$
        \end{itemize}
\end{itemize}
\textbf{Estimation Problem}
\begin{itemize}
    \item
        Given an observation sequence $O$, improve probability measures of models
    \item
        Baum-Wekch algorithm:
        \begin{itemize}
            \item
                TODO:....
        \end{itemize}
\end{itemize}
\subsection*{Application to Chord Recognition}
\textbf{Specification of Emission Probabilities}
\begin{itemize}
    \item
        To make HMM discrete (make observations coming from a finite output space) one can introduce a so-called codebook (set of prototypical vectors).\textbf{Quantization}, clustering
    \item
        Alternative: continuous HMMs, emission probability becomes continuous pdf
\end{itemize}
\textbf{Specification of Transition Probabilities}
\begin{itemize}
    \item
        Can be defined manually by expert
    \item
        Automatic approach:
        \begin{itemize}
            \item
                We assume the training set is represented by a sequence of frames and that each frame is labeled with ont of the states
            \item
                Next, count how often each of the possible chord transitions occurs in the training data
            \item
                $\mu(i,j) = $ number of transitions $\alpha_i \rightarrow \alpha_j$
            \item
                Transition probability matrix $A = (a_{ij})$ with $a_{ij} = \ffrac{\mu(i,j)}{\sum_k \mu(i,k)}$
            \item
                Dominated by diagonal matrix (self-transition probabilities): Chord durations occuring are much longer than the frame length
            \item
                Other elements, e.g. $\alpha_{i, i+7}$ are also much higher (in this example because change from tonic to dominant more probable) $\rightarrow$ secondary diagonals
            \item
                Indicates that transition probabilities depend on functional relations, independent of underlying key
            \item
                Because of this we can apply cyclic shifting and create transposition invariant matrix
            \item
                TODO: check ich ned
        \end{itemize}
\end{itemize}
\textbf{Evaluation}
\begin{itemize}
    \item
        Context-sensitive smoothing: recognizer tends to stay in the current chord rather than change to another one. Chord changes are only performed when the relatively low transitions probabilities are compensated by a substantial increase of emission probability
    \item
        High self transitions are the main factor
\end{itemize}

\newpage

\section{Lab Course}
\subsection{STFT and Chroma Features}
\begin{itemize}
    \item
        STFT: $X(m,k) = \sum_{n=0}^{N-1} x(n+mH)w(n)exp(-2 \pi i k n/N)$
    \item
        Spectrogram: $ Y(m,k) = |X(m,k)|^2$
    \item
        $T_{coef}(m) = \frac{m \cdot H}{F_s}$
    \item
        $F_{coef}(k) = \frac{k \cdot F_s}{N}$\\
    \item
        $(\Gamma_{\gamma} \dot Y)(m, k) = log(1 + \gamma \cdot Y(m,k))$, with $\Gamma_{\gamma}(v) = log(1 + \gamma \cdot v)$ \\
    \item
        Center frequency $F_{pitch}(p) = 2^{(p-69)/12} \cdot 440$
    \item
        Pitch set $P(p) = \{k: [0:K]: F_{pitch}(p-0.5) \leq F_{coef}(k) < F_{pitch}(p+0.5)\}$
    \item
        Log-frequency spectrogram: $Y_{LF}(m,p) = \sum_{k \in P(p)} |X(m,k)|^2$
    \item
        Chromagram: $C(m,c) = \sum_{\{ p \in [0:127] | p \; mod \; 12 = c\}} Y_{LF}(m,p)$
\end{itemize}
\subsection{Median Filtering}
\begin{itemize}
    \item
        Harmonically and percussively enhanced spectromgrams: $\tilde{Y}_p$ and $\tilde{Y}_h$: Median filter along time or frequency axis
    \item
        Binary mask $M_p(m,k)$: 1 if $\tilde{Y}_p \geq \tilde{Y}_h$, else zero (for percussive harmonic mask)
    \item
        Percussive STFT: $X_p = X \cdot M_p$
    \item
        HPSS for chroma enhancement: filter out percussive stuff
    \item
        HPSS for onset detection: filter out harmonic stuff
\end{itemize}

\end{document}
