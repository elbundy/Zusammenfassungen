\section*{17. The Expectation Maximization Algorithm}
\subsection*{Maximum Likelihood Estimation (ML estimation)}
\begin{itemize}
    \item
        Observations are assumued to be independent
    \item
        $\hat{\theta} = \underset{\theta}{argmax} p(x;\theta) = \underset{\theta}{argmax} log(p(x;\theta))$
    \item
        ML estimators for Gaussian distributions:
        \begin{align*}
            \{\hat{\mu}, \hat{\Sigma}\} = & \underset{\mu, \Sigma}{argmax} \prod_{i=1}^m p(x_i; \mu, \Sigma)\\ 
            = & \underset{\mu, \Sigma}{argmax} \sum_{i=1}^m log(p(x_i; \mu, \Sigma))\\
            = & \underset{\mu, \Sigma}{argmax} L(x_1, \dots, x_m; \mu, \Sigma)
        \end{align*}
        where $L$ is the log-likelihood function
        \begin{itemize}
            \item
                $\hat{\mu} = \ffrac{1}{m} \sum_{i=1}^m x_i$
            \item
                $\hat{\Sigma} = \ffrac{1}{m} \sum_{i=1}^m (x_i - \hat{\mu})(x_i - \hat{\mu})^T$
                
        \end{itemize}
\end{itemize}
\subsection*{Maximum a-posteriori estimation (MAP estimation)}
\begin{itemize}
    \item
        The probability density function of the parameters $p(\theta)$ to be estimated is known
    \item
        $\hat{\theta} = \underset{\theta}{argmax} p(\theta|x) = \dots = \underset{\theta}{argmax} log(p(\theta)) + log(p(x|\theta))$
\end{itemize}
\subsection*{Gaussian Mixture Models}
\begin{itemize}
    \item
        Given $m$ feature vectors in a $d$ dimensional space, find a set of $K$ multivariant Gaussian distributions that best represent the observations
    \item
        Unsupervised learning: It is not known which feature vectors are generated by which of the $K$ Gaussians, the desired output is, for each feature vector, an estimate of the probability that it is generated by distribution $k$
    \item
        GMM parameters to be estimated:
        \begin{itemize}
            \item
                $\mu_k$: the $K$ means
            \item
                $\Sigma_k$: the $K$ covariance matrices of size $d \times d$
            \item
                $p_k$: fraction of all feature vectors in component $k$ (priors)
            \item
                $p(k|i) \equiv p_{i,k}$: the $K$ probabilities for each of the $m$ feature vectors $x_i$ (posteriors)
        \end{itemize}
    \item
        Additional estimates:
        \begin{itemize}
            \item
                $p(x)$: probability distribution of observing a feature vector $x$
            \item
                $L$: overall log-likelihood function of the estimated parameter set
        \end{itemize}
\end{itemize}
\textbf{Expectation}
\begin{itemize}
    \item
        The key to the estimation problem is the overall log-likelihood objective function $L$:
        $$L = \sum_{i=1}^m log p(x_i)$$
    \item
        Split $p(x_i)$ into its contributions from the $K$ Gaussians:
        $$p(x_i) = \sum_{k=1}^K p_k \mathcal{N}(x_i|\mu_k,\Sigma_k)$$
    \item
        Posteriors for each of the $K$ contributions:
        $$p_{ik} \equiv p(k|i) = \ffrac{p_k \mathcal{N}(x_i|\mu_k,\Sigma_k)}{p(x_i)}$$
\end{itemize}
\textbf{Maximization}
\begin{itemize}
    \item
        How to maximize $L$ w.r.t. $\mu_k$, $\Sigma_k$ and $p_k$? $\rightarrow$ First derivatives of $L$
    \item
        The ML-estimates are:
        \begin{itemize}
            \item
                $\hat{\mu}_k = \ffrac{\sum_i p_{ik} x_i}{\sum_i p_{ik}}$
            \item
                $\hat{\Sigma}_k = \ffrac{\sum_i p_{ik} (x_i-\hat{\mu}_k) (x_i - \hat{\mu}_k)^T}{\sum_i p_{ik}}$
            \item
                $\hat{p}_k = \ffrac{1}{m} \sum_{i=1}^m p_{ik}$
        \end{itemize}
\end{itemize}
\textbf{EM-Algorithm}
\begin{lstlisting}[mathescape]
    Initialization: $\mu_k^{(0)}, \Sigma_k^{(0)}, p_k^{(0)}$
    $j \leftarrow 0$
        Expectation step: compute new values for $p_{ik}$, $L'$
        Maximization step: update values for $\mu_k^{(j)}, \Sigma_k^{(j)}, p_k^{(j)}$
        $j \leftarrow j+1$
    L is no longer changing
    Output: estimates $\hat{\mu}_k, \hat{\Sigma}_k, \hat{p}_k$
\end{lstlisting}

\subsection*{Missing Information Principle}
\begin{itemize}
    \item
        observable information = complete infromation - hidden information
    \item
        Theoretical foundation for Expectation Maximization Algorithm
    \item
        Joint probability of the events $x$ (observable) and $y$ (hidden) is: 
        $$p(x,y;\theta) = p(x;\theta)p(y|x;\theta)$$
        and thus
        $$p(x;\theta) = \ffrac{p(x,y;\theta)}{p(y|x;\theta)}$$
    \item
        The mathematical formulation of the MIP is (at iteration (i+1)):
        $$log(p(x;\theta^{(i+1)})) = log(p(x,y;\theta^{(i+1)})) - log(p(y|x;\theta^{(i+1)}))$$
    \item
        One can reformulate this into the key equation of the EM algorithm
        $$log(p(x;\theta^{(i+1)})) = Q(\theta^{(i)}; \theta^{(i+1)}) + H (\theta^{(i)}; \theta^{(i+1)})$$
        \begin{itemize}
            \item
                Kuhlback-Leibler Statistics
                $$ Q(\theta^{(i)}; \theta^{(i+1)}) = \int p(y|x;\theta^{(i)})log(p(x,y;\theta^{(i+1)}dy$$
            \item
                Entropy
                $$ H(\theta^{(i)}; \theta^{(i+1)}) = - \int p(y|x;\theta^{(i)})log(p(y|x;\theta^{(i+1)}dy$$
        \end{itemize}
    \item
        One can show that maximization of the Kullback-Leibler statistics can replace the optimization of the log-likelihood function
\begin{lstlisting}[mathescape]
    Initialization: $\theta^{(0)}$
    $j \leftarrow 0$
        Expectation step:
            $Q(\theta^{(i)}; \theta) = \int p(y|x;\theta^{(i)})log(p(x,y;\theta)) dy$
        Maximization step:
            $\theta^{(i+1)} \leftarrow \underset{\theta}{argmax} Q(\theta^{(i)}; \theta)$
        $j \leftarrow j+1$
    $\theta^{(i+1)} = \theta^{(i)}$
    Output: $\theta \leftarrow \theta^{(i)}$
\end{lstlisting}
\end{itemize}

\subsection*{Adendum}
\textbf{Advantages}
\begin{itemize}
    \item
        The maximum of the KL statistics is usually computed using zero crossings of the gradient
    \item
        Mostly we find closed form iteration schemes
    \item
        Iteration scheme is numerically robust and iterations have constant memory requirements
    \item
\end{itemize}
\textbf{Drawbacks}
\begin{itemize}
    \item
        Slow convergence
    \item
        Local optimization method, can get stuck in local minima
\end{itemize}
\textbf{Initialization of Priors}
\begin{itemize}
    \item
        Use prior knowledge
    \item
        If not available, assume uniform distribution
\end{itemize}

