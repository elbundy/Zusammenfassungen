\subsection*{Problem Formulation}
\subsubsection*{Hyperplanes}
\begin{itemize}
    \item
        Given: dot product space $H$ and set of patterns $x_{1}, \dots, x_{m} \in H$.
    \item
        Any hyperplane in $H$ can be written as 
        $$\{x \in H | \inner{w, x} + b = 0\}, w \in H, b \in \mathbb{R}$$
    \item
        $w$ is vector orthogonal to hyperplane.
    \item
        Canonical Hyperplanes: 
        The pair $(w,b) \in H \times \mathbb{R}$ is called a canonical form of the hyperplane with respect to $x_{1}, \dots, x_{m} \in H$ if it is scaled such that
        $$ min_{\; i=1, \dots, m \;} \abs{\inner{w, x_{i}} + b} = 1$$
        so the closest point to the hyperplane has a distance of \ffrac{1}{\norm{w}}.
    \item
        Decision function:
        \begin{align*}
        f_{w,b}: \; & H \rightarrow \{-1, +1\}\\
            & x \rightarrow f_{w,b}(x) = sgn(\inner{w,x}+b)
        \end{align*}
\end{itemize}

\subsubsection*{(Geometrical) Margin}
\begin{itemize}
    \item
        For a hyperplane $\{x \in H | \inner{w,x} +b = 0\}$, we call
        $$ p_{(w, b)}(x,y) = y(\inner{w,x}+b)/\norm{w}$$
        the geometrical margin of the point $(x,y) \in H \times \{-1, +1\}$.

    \item
        The minimum value
        $$ p_{(w, b)} = min_{\;i=1,\dots,m} \;p_{(w, b)}(x_{i},y_{i})$$
        shall be called the geometrical margin of the training set.
    \item
        For canonical hyperplanes, the margin is $\ffrac{1}{\norm{w}}$.
\end{itemize}

\subsubsection*{Optimal Margin Hyperplanes}
\begin{itemize}
    \item
        Set of examples $(x_1, y_1), \dots, (x_m, y_m)$.
    \item
        Goal: Find hyperplane with maximum margin, which satisfies $f_{w,b}(x_i) = y_i$ with
        $$f_{w,b}(x) = sgn(\inner{w,x}+b)$$
        for all examples.
    \item
        Canonicality implies $y_i(\inner{x, x_i}+b) \geq 1$
    \item
        Solve following problem:
        \begin{align*}
            maximize \; \; & \ffrac{1}{\norm{w}}\\
            subject \; to \; \; & y_i(\inner{w, x_i}+b) \geq 1 %\;\; \forall i=1, \dots, m
        \end{align*}
        which is equal to
        \begin{align*}
            minimize \; \; & \ffrac{1}{2}\norm{w}^2\\
            subject \; to \; \; & y_i(\inner{w, x_i}+b) \geq 1 %\;\; \forall i=1, \dots, m
        \end{align*}
    \item
        Soft-Margin case:
        \begin{align*}
            minimize \; \; & \ffrac{1}{2}\norm{w}^2 + \mu \sum_i \xi_i\\
            subject \; to \; \; & -(y_i(\inner{w, x_i}+b)-1+\xi_i) \leq 0 \\%\;\; \forall i=1, \dots, m\\
            & -\xi_i \leq 0 \;\; %\forall i=1, \dots, m
        \end{align*}

\end{itemize}

\subsubsection*{Dual form of the optimization problem (Hard-Margin case)}
\begin{itemize}
    \item
        Lagrangian:
        $$L(w,b,\alpha) = \ffrac{1}{2}\norm{w}^2-\sum_{i=1}^{m}\alpha_i(y_i(\inner{w,x_i}+b)-1)$$
    \item
        Karush-Kuhn-Tucker: Gradient of the Lagrangian must be zero at optimum
    \item
        $$\ffrac{\delta}{\delta b}L(w,b,\alpha) = 0, \;\; \ffrac{\delta}{\delta w}L(w,b,\alpha) = 0$$
        yields
        $$\sum_{i=1}^{m}\alpha_i y_i = 0, \;\; w = \sum_{i=1}^{m}\alpha_i y_i x_i$$
    \item
        Substituting the conditions for extremum we arrive at the dual form of the optimization problem:
        \begin{align*}
            maximize \; \; & W(\alpha) = \sum_{i=1}^{m}\alpha_i - \ffrac{1}{2}\sum_{i,j=1}^{m}\alpha_i \alpha_j y_i y_j \inner{x_i,x_j}\\
            subject \; to \; \; & \alpha_i \geq 0, i=1,\dots,m \\%\;\; \forall i=1, \dots, m
            & \sum_{i=1}^{m} \alpha_i y_i = 0
        \end{align*}
    \item
        Decision function:
        $$f(x) = sgn(\sum_{i=1}^{m}\alpha_i y_i \inner{x,x_i}+b)$$
    \item
        Karush-Kuhn-Tucker: $\alpha_i[y_i(\inner{w,x_i}+b)-1]=0 \;\;\forall i=1,\dots ,m$\\
        Patterns $x_i$, for which $a_i > 0$ are called \textit{Support Vectors}.
\end{itemize}

\subsection*{Kernels}
\begin{itemize}
    \item
        \textit{Problem}: SVMs can only be used, when classes are linearly seperable
    \item
        \textit{Solution}: Apply feature transform $\phi: \mathbb{R}^{d} \rightarrow \mathbb{R}^{D}, D \geq d$, such that the resulting features $\phi(x_i)$ are linearly seperable.
    \item
        Additionally, feature vectors only appear in inner products, both in the learning and the classification phase.
    \item
        \textit{Idea:} Introduce kernels $k(x, x') = \inner{\phi(x), \phi(x')}$, which calculate the values of inner products of transformed feature vectors.
    \item
        \textit{Kernel trick}: Kernels can be defined in a way, such that the transform $\phi$ doesn't even have to be computed, which saves computation time.
    \item
        \textbf{Example:}
        $$x = (x_1, x_2),\;\; \phi(x) = (x_1^2, x_2^2, x_1 x_2, x_2 x_1)$$
        $$\inner{\phi(x), \phi(x')} = x_1^2 x_1 '^2 + x_2^2 x_2 '^2 + 2 x_1 x_2 x_1' x_2' = \inner{x, x'}^2$$
        so
        $$ k(x, x') = \inner{\phi(x), \phi(x')} = \inner{x,x'}^2$$
    \item
        Kernel matrix:
        $$ K = [K_{ij}], \; \; K_{i,j} = \inner{\phi(x_i), \phi(x_j)}$$
    \item
        Typical kernel functions:
        \begin{itemize}
            \item
                Linear: $k(x, x') = \inner{x, x'}$
            \item
                Polynomial: $k(x, x') = (\inner{x,x'} + 1)^d$
            \item
                Laplacian radial basis function: $k(x, x') = e^{-\ffrac{\norm{x-x'}_1}{\sigma^2}}$
            \item
                Gaussian radial basis function: $k(x, x') = e^{-\ffrac{\norm{x-x'}_2^2}{\sigma^2}}$
            \item
                Sigmoid kernel: $k(x, x') = tanh(\alpha \inner{x, x'} + \beta)$
        \end{itemize}
\end{itemize}
